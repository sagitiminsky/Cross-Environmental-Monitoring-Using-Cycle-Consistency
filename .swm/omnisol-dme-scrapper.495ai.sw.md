---
id: 495ai
name: Omnisol (dme) scrapper
file_version: 1.0.2
app_version: 0.7.1-2
file_blobs:
  CellEnMon/libs/scrappers/dme_scrapper/scrapper.py: c03e64ea9e20d10eeec96eed0acdfa1ee27af37a
---

Hi and welcome to the Omnisol (dme) scrapper. I will quickly explain how to extract information for the Omnisol Dailey Measurement Explorer, or in short dme.

TL;DR python3 `ðŸ“„ CellEnMon/libs/scrappers/dme_scrapper/scrapper.py`

I've created a short `ðŸ“„ CellEnMon/libs/scrappers/dme_scrapper/requirements.txt` requirements file, in case you do not want to install torch for example, you are only intrested in running this script.

<br/>

The `ðŸ“„ CellEnMon/config.py` is basically the controller for this. It defines the criteria for the scraping.
<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->
### ðŸ“„ CellEnMon/libs/scrappers/dme_scrapper/scrapper.py
```python
â¬œ 1      import time
ðŸŸ© 2      import CellEnMon.config as config
â¬œ 3      import os
â¬œ 4      import shutil
â¬œ 5      import io
```

<br/>

The script serves multiple purpuses:

<br/>

you can select

DOWNLOAD - only to Download the files into your Downloads folder  
EXTRACT - extract the downloaded files from the Downloads folder and merge all the information based on the station  
UPLOAD - upload the extracted information to gcp
<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->
### ðŸ“„ CellEnMon/libs/scrappers/dme_scrapper/scrapper.py
```python
â¬œ 22     from CellEnMon.libs.power_law.power_law import PowerLaw
â¬œ 23     from google.cloud import storage
â¬œ 24     
ðŸŸ© 25     SELECTOR = ['EXTRACT', 'UPLOAD'] # DOWNLOAD | EXTRACT | UPLOAD
â¬œ 26     
â¬œ 27     
â¬œ 28     ## Setting credentials using the downloaded JSON file
```

<br/>

If you will try to upload the files, make sure to have the proper creds to do so
<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->
### ðŸ“„ CellEnMon/libs/scrappers/dme_scrapper/scrapper.py
```python
â¬œ 26     
â¬œ 27     
â¬œ 28     ## Setting credentials using the downloaded JSON file
ðŸŸ© 29     if "UPLOAD" in SELECTOR:
ðŸŸ© 30         path = config.bucket_creds
ðŸŸ© 31         client = storage.Client.from_service_account_json(json_credentials_path=path)
ðŸŸ© 32     
â¬œ 33     class DME_Scrapper_obj:
â¬œ 34     
â¬œ 35         def __init__(self, mock=None):
```

<br/>

One of the problems I encoutered is the fact the that "Download Metadata" button, does not work. So I had to manually export the table from dme. I did it by pressing the "End" option using the script, so that it will bring to the end of the table - so that I could download the information.
<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->
### ðŸ“„ CellEnMon/libs/scrappers/dme_scrapper/scrapper.py
```python
â¬œ 238            if len(os.listdir(self.root_download)) - prev_number_of_files == delta:
â¬œ 239                return
â¬œ 240    
ðŸŸ© 241        def download_data(self, link_name,start_day, end_day=None):
ðŸŸ© 242            try:
ðŸŸ© 243    
ðŸŸ© 244                # download data
ðŸŸ© 245                self.browser.find_element_by_xpath(self.xpaths['xpath_download']).click()
ðŸŸ© 246                WebDriverWait(self.browser, self.delay).until(
ðŸŸ© 247                    EC.element_to_be_clickable((By.XPATH, self.xpaths['xpath_download'])))
ðŸŸ© 248    
ðŸŸ© 249                time.sleep(1)
ðŸŸ© 250    
ðŸŸ© 251                # download metadata
ðŸŸ© 252                ActionChains(self.browser).context_click(
ðŸŸ© 253                    self.browser.find_element_by_xpath('//*[@id="dailies"]/div/div[2]/div[1]/div[3]')).send_keys(Keys.END).perform()
ðŸŸ© 254    
ðŸŸ© 255                ActionChains(self.browser).context_click(
ðŸŸ© 256                    self.browser.find_element_by_xpath('//*[@id="dailies"]/div/div[2]/div[1]/div[3]')).perform()
ðŸŸ© 257                self.browser.find_element_by_xpath('//*[ @ id = "dailies"]/div/div[6]/div/div/div[5]/span[2]').click()
ðŸŸ© 258    
ðŸŸ© 259                self.browser.find_element_by_xpath(self.xpaths['xpath_metadata_download']).click()
â¬œ 260    
â¬œ 261    
â¬œ 262    
```

<br/>

Meta data is going to be saved into the file's name
<!-- NOTE-swimm-snippet: the lines below link your snippet to Swimm -->
### ðŸ“„ CellEnMon/libs/scrappers/dme_scrapper/scrapper.py
```python
â¬œ 211                if valid:
â¬œ 212                    # link_frequency=merged_df_dict[link_name]['frequency'][0]
â¬œ 213                    # link_polarization=merged_df_dict[link_name]['polarization'][0]
â¬œ 214                    # link_length=merged_df_dict[link_name]['length'][0]
â¬œ 215                    link_tx_longitude=merged_df_dict[link_name]['tx_longitude'][0]
â¬œ 216                    link_tx_latitude=merged_df_dict[link_name]['tx_latitude'][0]
â¬œ 217                    link_rx_longitude=merged_df_dict[link_name]['rx_longitude'][0]
â¬œ 218                    link_rx_latitude=merged_df_dict[link_name]['rx_latitude'][0]
â¬œ 219                    tx_name,rx_name=link_name.split("-")
ðŸŸ© 220                    metadata=f"{tx_name}-{link_tx_latitude}-{link_tx_longitude}-{rx_name}-{link_rx_latitude}-{link_rx_longitude}"
â¬œ 221                    link_file_name= f"{config.dme_root_files}/raw/{metadata}.csv"
â¬œ 222    
â¬œ 223                    try:
```

<br/>

This file was generated by Swimm. [Click here to view it in the app](https://app.swimm.io/repos/Z2l0aHViJTNBJTNBQ2VsbEVuTW9uLVJlc2VhcmNoJTNBJTNBc2FnaXRpbWluc2t5/docs/495ai).